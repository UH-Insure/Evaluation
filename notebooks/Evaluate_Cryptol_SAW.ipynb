{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74dc0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/UH-Insure/Evaluation.git\n",
    "%cd Evaluation\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dff960",
   "metadata": {},
   "source": [
    "# Single-Model Evaluation (Cryptol/SAW)\n",
    "\n",
    "Choose Model A for Josh's model, Model B for Talha's model\n",
    "\n",
    "Pipeline: install SAW/Cryptol → load tasks → generate k candidates → compile with `cryptol` → optional verify with `saw` → metrics/pass@k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618971c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/UH-Insure/crypto-c.git\n",
    "%cd crypto-c\n",
    "!chmod +x scripts/colab_setup.sh\n",
    "!bash scripts/colab_setup.sh\n",
    "!bash scripts/run-all-saw.sh\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a2e49",
   "metadata": {
    "id": "choose_model"
   },
   "outputs": [],
   "source": [
    "MODEL_A = \"j05hr3d/peft-FT-3-Coder-30b-v3\"  \n",
    "MODEL_B = \"tam2003/peft-FT-3-Coder-30b-v3\"  \n",
    "CHOOSE = \"A\"  \n",
    "CUSTOM  = \"\" \n",
    "TASKS_PATH = \"eval/prompts/tasks.jsonl\" \n",
    "K = 5 \n",
    "MAX_NEW_TOKENS_DEFAULT = 512 \n",
    "SEED = 42  \n",
    "USE_DRIVE = False  \n",
    "\n",
    "def pick(ch, a, b, c):\n",
    "  if ch == \"A\": return a\n",
    "  if ch == \"B\": return b\n",
    "  return c.strip() or a\n",
    "\n",
    "MODEL_ID = pick(CHOOSE, MODEL_A, MODEL_B, CUSTOM)\n",
    "print(\"[+] Using model:\", MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd43d90",
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "if USE_DRIVE:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  BASE_OUT = '/content/drive/MyDrive/uh-crypto-llm-eval'\n",
    "else:\n",
    "  BASE_OUT = '../outputs'\n",
    "os.makedirs(BASE_OUT, exist_ok=True)\n",
    "print(\"[+] Outputs ->\", BASE_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77fb2cb",
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "!pip -q install --upgrade transformers accelerate datasets pandas tiktoken orjson\n",
    "!bash ../scripts/install_saw_cryptol.sh\n",
    "!saw --version || true\n",
    "!cryptol --version || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f64408",
   "metadata": {
    "id": "helpers"
   },
   "outputs": [],
   "source": [
    "import json, pathlib, subprocess, uuid, random\n",
    "from datetime import datetime\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "def load_tasks(path:str):\n",
    "  tasks=[]\n",
    "  with open(path,'r') as f:\n",
    "    for line in f:\n",
    "      if line.strip(): tasks.append(json.loads(line))\n",
    "  return tasks\n",
    "\n",
    "def now_tag(): return datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "def try_compile_with_cryptol(code:str)->bool:\n",
    "  tmp = pathlib.Path('tmp_cryptol'); tmp.mkdir(exist_ok=True)\n",
    "  p = tmp / f\"gen_{uuid.uuid4().hex}.cry\"\n",
    "  p.write_text(code, encoding='utf-8')\n",
    "  cmd = [\"bash\",\"-lc\", f\"echo ':l {p}' | cryptol\"]\n",
    "  try:\n",
    "    out = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=60)\n",
    "    return out.returncode == 0 and b\"Type checked.\" in out.stdout\n",
    "  except Exception:\n",
    "    return False\n",
    "\n",
    "def run_harness_if_any(h)->bool:\n",
    "  if not h: return False\n",
    "  if not pathlib.Path(h).exists(): return False\n",
    "  try:\n",
    "    out = subprocess.run([\"bash\",\"-lc\", f\"saw {h}\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=300)\n",
    "    return out.returncode == 0\n",
    "  except Exception:\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6398e7c9",
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "print(\"[+] Model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a797461",
   "metadata": {
    "id": "gen"
   },
   "outputs": [],
   "source": [
    "def generate_code(prompt:str, max_new_tokens:int=512, temperature:float=0.7, top_p:float=0.95, do_sample:bool=True):\n",
    "  input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(model.device)\n",
    "  with torch.no_grad():\n",
    "    out = model.generate(\n",
    "      input_ids,\n",
    "      max_new_tokens=max_new_tokens,\n",
    "      temperature=temperature,\n",
    "      top_p=top_p,\n",
    "      do_sample=do_sample,\n",
    "      pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "  text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "  return text[len(prompt):].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec4656",
   "metadata": {
    "id": "run_eval"
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "tasks = load_tasks(TASKS_PATH)\n",
    "tag = now_tag()\n",
    "outdir = os.path.join(BASE_OUT, tag)\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "raw_path = os.path.join(outdir, 'raw_generations.jsonl')\n",
    "metrics_path = os.path.join(outdir, 'metrics.json')\n",
    "\n",
    "total = compiles = verified = passk_compile = passk_verify = 0\n",
    "\n",
    "with open(raw_path, 'w') as gf:\n",
    "  for t in tasks:\n",
    "    k = int(t.get('k', K))\n",
    "    compiled_any = False\n",
    "    verified_any = False\n",
    "    for i in range(k):\n",
    "      total += 1\n",
    "      code = generate_code(t['prompt'], max_new_tokens=int(t.get('max_new_tokens', MAX_NEW_TOKENS_DEFAULT)))\n",
    "      ok_compile = try_compile_with_cryptol(code)\n",
    "      if ok_compile:\n",
    "        compiles += 1; compiled_any = True\n",
    "      ok_verify = False\n",
    "      if t.get('harness'):\n",
    "        ok_verify = run_harness_if_any(t['harness'])\n",
    "        if ok_verify:\n",
    "          verified += 1; verified_any = True\n",
    "      row = {\"task_id\": t[\"task_id\"], \"attempt\": i, \"model\": MODEL_ID, \"prompt\": t[\"prompt\"], \"code\": code, \"compiled\": ok_compile, \"verified\": ok_verify}\n",
    "      gf.write(json.dumps(row) + \"\\n\")\n",
    "    if compiled_any: passk_compile += 1\n",
    "    if verified_any: passk_verify += 1\n",
    "\n",
    "metrics = {\n",
    "  \"total_attempts\": total,\n",
    "  \"compile_successes\": compiles,\n",
    "  \"verify_successes\": verified,\n",
    "  \"compile_rate\": compiles/total if total else 0.0,\n",
    "  \"verify_rate\": verified/total if total else 0.0,\n",
    "  \"pass@k_compile\": passk_compile/len(tasks) if tasks else 0.0,\n",
    "  \"pass@k_verify\": passk_verify/len(tasks) if tasks else 0.0,\n",
    "  \"model\": MODEL_ID,\n",
    "  \"timestamp\": tag,\n",
    "}\n",
    "with open(metrics_path, 'w') as mf: mf.write(json.dumps(metrics))\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd986698",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Point `harness` in `tasks.jsonl` to crypto-c SAW scripts.\n",
    "- Keep SAW/Cryptol versions aligned with Docker for reproducibility."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Evaluate_Cryptol_SAW.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

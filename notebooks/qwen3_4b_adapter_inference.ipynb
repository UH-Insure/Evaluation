{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2495868c",
      "metadata": {
        "id": "2495868c"
      },
      "source": [
        "\n",
        "# Qwen3‑4B + PEFT Adapter Inference (Boilerplate)\n",
        "\n",
        "This notebook loads the **Qwen3‑4B** base model and applies your **LoRA/PEFT** adapters from Hugging Face\n",
        "(`j05hr3d/SFT-Qwen3-4B`) for inference. It uses the model's **chat template** so prompts are formatted exactly as expected.\n",
        "\n",
        "**What you get:**\n",
        "- Configurable base model id (default: `Qwen/Qwen3-4B-Instruct`)\n",
        "- Load adapters via **`AutoPeftModelForCausalLM.from_pretrained`** (recommended) or by loading base + `PeftModel.from_pretrained`\n",
        "- Example chat with a **system** message and a **user** message\n",
        "- Optional `.merge_and_unload()` to bake LoRA weights for slightly faster inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c21f7968",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c21f7968",
        "outputId": "2c3743f6-6b6d-4d1f-9f8c-8f704bc6b23c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repo 'Colab-Training' exists, pulling latest changes...\n",
            "HEAD is now at 6e1eba3 F\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (4/4), 370 bytes | 370.00 KiB/s, done.\n",
            "From https://github.com/UH-Insure/Colab-Training\n",
            "   6e1eba3..8cda225  main       -> origin/main\n",
            "Updating 6e1eba3..8cda225\n",
            "Fast-forward\n",
            " data/evals.jsonl | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "REPO_URL=\"https://github.com/UH-Insure/Evaluation.git\"\n",
        "REPO=\"Colab-Training\"\n",
        "\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "# If repo exists, update it; otherwise, clone fresh\n",
        "if os.path.exists(REPO):\n",
        "    print(f\"Repo '{REPO}' exists, pulling latest changes...\")\n",
        "    os.chdir(REPO)\n",
        "    !git reset --hard HEAD   # optional: discard local changes\n",
        "    !git pull\n",
        "else:\n",
        "    print(f\"Cloning repo '{REPO}'...\")\n",
        "    !git clone \"$REPO_URL\" \"$REPO\"\n",
        "    os.chdir(REPO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "6d904ead",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d904ead",
        "outputId": "9f4b774d-a76f-495d-f5f6-4d9486bc533d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda | dtype: torch.bfloat16\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import AutoPeftModelForCausalLM, PeftModel\n",
        "\n",
        "# ----------------- USER CONFIG -----------------\n",
        "BASE_MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"     # Base model; change if you fine-tuned a different Qwen3 variant\n",
        "ADAPTER_REPO  = \"j05hr3d/SFT-Qwen3-4B\"       # Your adapter repo on Hugging Face\n",
        "TRUST_REMOTE_CODE = True                     # Qwen chat template typically needs this\n",
        "USE_AUTOPEFT = True                          # True: load adapters directly from the adapter repo\n",
        "MERGE_AND_UNLOAD = False                     # True to merge LoRA weights into the base for faster inference\n",
        "DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "GEN_KW = dict(\n",
        "    max_new_tokens=1024,     # enough to hold the whole `cryptol` block\n",
        "    do_sample=True,\n",
        "    temperature=0.7,        # lower than 0.7 → more deterministic\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "# -----------------------------------------------\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device, \"| dtype:\", DTYPE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "d0922c58",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "d0922c58",
        "outputId": "ab171b11-9434-484f-c6fa-8c23acc2e570"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded tokenizer. EOS: 151645 PAD: 151643\n"
          ]
        }
      ],
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    BASE_MODEL_ID if not USE_AUTOPEFT else ADAPTER_REPO,  # adapter repos often include correct tokenizer config\n",
        "    use_fast=True,\n",
        "    trust_remote_code=TRUST_REMOTE_CODE,\n",
        ")\n",
        "# Ensure pad token exists\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "print(\"Loaded tokenizer. EOS:\", tokenizer.eos_token_id, \"PAD:\", tokenizer.pad_token_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "3f7091af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "7c32e0c7b4b34cb4b1682a165550b353",
            "d0166372abc94ebe95593adfc49e0689",
            "14deeb7fd2d2486b9d48906579fc160f",
            "863f9e09a09e46639953c1318fd86dbb",
            "f3a99c35ef2a4454a817685815f6b1fb",
            "268fb6330a7f41218288287c0d36f551",
            "0af5b54b315b409386adc15531211ac0",
            "b57a811879d245d1b703d78b2bbdf373",
            "8107004eddc4460b8f36472a6fc9a80d",
            "b034d39ccaf44f829cb5e18a85f61205",
            "11f9dfce3a7849b48f55de97ba7a7fa7"
          ]
        },
        "id": "3f7091af",
        "outputId": "0210fb60-0616-4406-c64b-59e87ac9c133"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c32e0c7b4b34cb4b1682a165550b353",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['alora_invocation_tokens', 'arrow_config', 'ensure_weight_tying', 'peft_version'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model ready.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Fallback: load base model, then apply adapters\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    torch_dtype=DTYPE,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=TRUST_REMOTE_CODE,\n",
        ")\n",
        "model = PeftModel.from_pretrained(base, ADAPTER_REPO)\n",
        "\n",
        "# Optionally merge LoRA weights for faster inference (uses more RAM/VRAM)\n",
        "if MERGE_AND_UNLOAD and hasattr(model, \"merge_and_unload\"):\n",
        "    model = model.merge_and_unload()\n",
        "    print(\"Merged LoRA weights into the base model.\")\n",
        "\n",
        "# Make sure pad token id is set for generation\n",
        "if getattr(model.config, \"pad_token_id\", None) is None:\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "_ = model.eval()\n",
        "print(\"Model ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "d3f76673",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3f76673",
        "outputId": "020984b1-6350-43c3-ec01-0f399162945f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting eval suite at 2025-11-14_19:01:54, 4 tasks to process.\n",
            "\n",
            "=== Task 1 ===\n",
            "\n",
            "[PROMPT BEGIN]\n",
            "[SYSTEM]\n",
            "Return exactly ONE fenced code block labeled `cryptol` and nothing else (no prose before/after).\n",
            "\n",
            "[USER]\n",
            "### Instruction:\n",
            "Write a Cryptol function that implements the tasks described below.\n",
            "\n",
            "### Request:\n",
            "Task: Implement a function named `xor8` which takes two 8-bit words and returns their bitwise XOR. Also provide a constant `zero8` equal to 0 (8-bit).\n",
            "\n",
            "[PROMPT END]\n",
            "\n",
            "[GENERATED BEGIN]\n",
            "```cryptol\n",
            "xor8 : [8] -> [8] -> [8]\n",
            "xor8 x y = x ^ y\n",
            "\n",
            "zero8 : [8]\n",
            "zero8 = 0\n",
            "```\n",
            "[GENERATED END]\n",
            "\n",
            "\n",
            "=== Task 2 ===\n",
            "\n",
            "[PROMPT BEGIN]\n",
            "[SYSTEM]\n",
            "Return exactly ONE fenced code block labeled `cryptol` and nothing else (no prose before/after).\n",
            "\n",
            "[USER]\n",
            "### Instruction:\n",
            "Write a Cryptol function that implements the tasks described below.\n",
            "\n",
            "### Request:\n",
            "Task: Implement a function named `matrix` which takes no input and returns a 3 x 3 matrix of tuples. For the function use a nested comprehension to write an expression to produce a 3 × 3 matrix (as a sequence of sequences), such that the ijth entry contains the value (i, j).\n",
            "\n",
            "[PROMPT END]\n",
            "\n",
            "[GENERATED BEGIN]\n",
            "```cryptol\n",
            "matrix : {a} (fin a) => [a][a]tup\n",
            "matrix = [ [ (i, j) | j <- [0 .. a-1] ] | i <- [0 .. a-1] ]\n",
            "```\n",
            "[GENERATED END]\n",
            "\n",
            "\n",
            "=== Task 3 ===\n",
            "\n",
            "[PROMPT BEGIN]\n",
            "[SYSTEM]\n",
            "Return exactly ONE fenced code block labeled `cryptol` and nothing else (no prose before/after).\n",
            "\n",
            "[USER]\n",
            "### Instruction:\n",
            "Write a Cryptol function that implements the tasks described below.\n",
            "\n",
            "### Request:\n",
            "Task: Implement a Caesar cipher. Provide a function `encrypt key text` that performs a right circular shift on finite length strings and a function `decrypt key text` that performs a left circular shift on finite length strings.\n",
            "\n",
            "[PROMPT END]\n",
            "\n",
            "[GENERATED BEGIN]\n",
            "```cryptol\n",
            "rightShift : {a} [a] -> [a]\n",
            "rightShift xs = take (xs # 0)\n",
            "\n",
            "leftShift : {a} [a] -> [a]\n",
            "leftShift xs = drop (xs # 0)\n",
            "\n",
            "encrypt : {a} [a] -> [a] -> [a]\n",
            "encrypt key text = rightShift (key # text)\n",
            "\n",
            "decrypt : {a} [a] -> [a] -> [a]\n",
            "decrypt key text = leftShift (key # text)\n",
            "```\n",
            "[GENERATED END]\n",
            "\n",
            "\n",
            "=== Task 4 ===\n",
            "\n",
            "[PROMPT BEGIN]\n",
            "[SYSTEM]\n",
            "Return exactly ONE fenced code block labeled `cryptol` and nothing else (no prose before/after).\n",
            "\n",
            "[USER]\n",
            "### Instruction:\n",
            "Write a Cryptol property that tests the function described below.\n",
            "\n",
            "### Request:\n",
            "Task: Using ONLY the functions and constraints provided, create a monomorphic Cryptol property named `equivalent` that proves decrypt inverts encrypt for a 512-byte plaintext.\n",
            "\n",
            "### Additional setup code:\n",
            "```cryptol\n",
            "encryptChar : [8] -> [8] -> [8]\n",
            "encryptChar key c = (c + key)\n",
            "decryptChar : [8] -> [8] -> [8]\n",
            "decryptChar key c = (c - key)\n",
            "encrypt : {n} [8] -> [n][8] -> [n][8]\n",
            "encrypt key txt = [ encryptChar key c | c <- txt ]\n",
            "decrypt : {n} [8] -> [n][8] -> [n][8]\n",
            "decrypt key txt = [ decryptChar key c | c <- txt ]\n",
            "```\n",
            "[PROMPT END]\n",
            "\n",
            "[GENERATED BEGIN]\n",
            "```cryptol\n",
            "encryptChar : [8] -> [8] -> [8]\n",
            "encryptChar key c = (c + key)\n",
            "decryptChar : [8] -> [8] -> [8]\n",
            "decryptChar key c = (c - key)\n",
            "encrypt : {n} [8] -> [n][8] -> [n][8]\n",
            "encrypt key txt = [ encryptChar key c | c <- txt ]\n",
            "decrypt : {n} [8] -> [n][8] -> [n][8]\n",
            "decrypt key txt = [ decryptChar key c | c <- txt ]\n",
            "\n",
            "equivalent : [8] -> [512][8] -> Bool\n",
            "equivalent key txt = decrypt key (encrypt key txt) == txt\n",
            "```\n",
            "[GENERATED END]\n",
            "\n",
            "\n",
            "2025-11-14_19-02-18\n",
            "Done processing all evals.\n"
          ]
        }
      ],
      "source": [
        "from src.eval_suite import Config, run_eval_suite\n",
        "import pandas as pd\n",
        "\n",
        "config = Config(\n",
        "    MODEL_ID=ADAPTER_REPO,   # purely informational for local runs\n",
        "    EVALS_PATH=\"data/evals.jsonl\",\n",
        "    TEMP_FILE=\"data/generated.txt\",\n",
        "    SYSTEM_PROMPT=\"Return exactly ONE fenced code block labeled `cryptol` and nothing else (no prose before/after).\",\n",
        ")\n",
        "\n",
        "eval_df = pd.read_json(config.EVALS_PATH, lines=True)\n",
        "\n",
        "def generate_cryptol(messages) -> str:\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            **GEN_KW,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "\n",
        "    generated = outputs[0, inputs.shape[-1]:]  # only the new tokens\n",
        "    text = tokenizer.decode(generated, skip_special_tokens=True)\n",
        "    return text\n",
        "\n",
        "# ---- 3. Run the eval suite using the local model ----\n",
        "run_eval_suite(eval_df, config, execute=False, generate_fn=generate_cryptol)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "607e839b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "607e839b",
        "outputId": "99dfe725-29f2-4c76-e516-8d03fedb527d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant:\n",
            " ```cryptol\n",
            "xor8 : [8] -> [8] -> [8]\n",
            "xor8 x y = x ^ y\n",
            "\n",
            "zero8 : [8]\n",
            "zero8 = 0\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# Define conversation in the model's expected format\n",
        "messages = [\n",
        "   {\"role\": \"system\", \"content\": \"Return exactly ONE fenced code block labeled `cryptol` and nothing else (no prose before/after).\"},\n",
        "    {\"role\": \"user\", \"content\": \"Implement a function named `xor8` which takes two 8-bit words and returns their bitwise XOR. Also provide a constant `zero8` equal to 0 (8-bit).\"},\n",
        "]\n",
        "\n",
        "# Build inputs using the chat template and ask the model to generate the assistant's reply\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,  # append assistant header so the model continues as assistant\n",
        "    tokenize=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        **GEN_KW,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "generated = outputs[0, inputs.shape[-1]:]\n",
        "text = tokenizer.decode(generated, skip_special_tokens=True)\n",
        "print(\"Assistant:\\n\", text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "9cd93ec5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cd93ec5",
        "outputId": "281b396f-5ef6-47fc-8a4c-4ad2736f6aea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An Echo State Network (ESN) is a type of recurrent neural network (RNN) that uses a fixed, randomly initialized reservoir of neurons with fixed connection weights. During training, only the weights connecting the reservoir to the output layer are adjusted, enabling the network to learn complex temporal patterns from input sequences.\n"
          ]
        }
      ],
      "source": [
        "def chat(system_prompt: str, user_prompt: str, **gen_kwargs):\n",
        "    msgs = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "    model_inputs = tokenizer.apply_chat_template(\n",
        "        msgs, add_generation_prompt=True, tokenize=True, return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(model_inputs, **({**GEN_KW, **gen_kwargs}), pad_token_id=tokenizer.pad_token_id)\n",
        "    completion = tokenizer.decode(out[0, model_inputs.shape[-1]:], skip_special_tokens=True)\n",
        "    return completion\n",
        "\n",
        "print(chat(\n",
        "    \"You are a precise software assistant.\",\n",
        "    \"Explain what an Echo State Network is in two sentences.\"\n",
        "))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f8a8c26",
      "metadata": {
        "id": "8f8a8c26"
      },
      "source": [
        "\n",
        "## Notes\n",
        "- If `AutoPeftModelForCausalLM.from_pretrained(ADAPTER_REPO)` fails because the adapter repo doesn't declare the base,\n",
        "  set `USE_AUTOPEFT=False` to load `BASE_MODEL_ID` first, then apply `PeftModel.from_pretrained(BASE_MODEL_ID, ADAPTER_REPO)`.\n",
        "- Keep `trust_remote_code=True` for Qwen so the tokenizer/model expose the correct **chat template**.\n",
        "- If you see CUDA OOM, consider running on CPU (`device_map=None`) or reducing `max_new_tokens` and using a smaller batch.\n",
        "- `MERGE_AND_UNLOAD=True` can speed up inference at the cost of more memory.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0af5b54b315b409386adc15531211ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11f9dfce3a7849b48f55de97ba7a7fa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14deeb7fd2d2486b9d48906579fc160f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b57a811879d245d1b703d78b2bbdf373",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8107004eddc4460b8f36472a6fc9a80d",
            "value": 3
          }
        },
        "268fb6330a7f41218288287c0d36f551": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c32e0c7b4b34cb4b1682a165550b353": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0166372abc94ebe95593adfc49e0689",
              "IPY_MODEL_14deeb7fd2d2486b9d48906579fc160f",
              "IPY_MODEL_863f9e09a09e46639953c1318fd86dbb"
            ],
            "layout": "IPY_MODEL_f3a99c35ef2a4454a817685815f6b1fb"
          }
        },
        "8107004eddc4460b8f36472a6fc9a80d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "863f9e09a09e46639953c1318fd86dbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b034d39ccaf44f829cb5e18a85f61205",
            "placeholder": "​",
            "style": "IPY_MODEL_11f9dfce3a7849b48f55de97ba7a7fa7",
            "value": " 3/3 [00:02&lt;00:00,  1.17s/it]"
          }
        },
        "b034d39ccaf44f829cb5e18a85f61205": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b57a811879d245d1b703d78b2bbdf373": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0166372abc94ebe95593adfc49e0689": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_268fb6330a7f41218288287c0d36f551",
            "placeholder": "​",
            "style": "IPY_MODEL_0af5b54b315b409386adc15531211ac0",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f3a99c35ef2a4454a817685815f6b1fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
